# ü§ñ Model Switcher Feature Guide

## Overview

The Model Switcher is an intelligent dropdown component integrated into the sidebar that allows users to seamlessly switch between different AI models during their conversation. It provides optimal AI responses by routing queries to the most appropriate model based on the conversation type.

## ‚ú® Features

### üéØ **Intelligent Model Selection**
- **Real-time switching** between different AI models
- **Automatic model routing** based on conversation type
- **User-specific configurations** for personalized experience
- **Visual indicators** showing current model and status

### üîß **Supported Models**

#### **Google Models**
- **Gemini Flash** - Fast responses with web search capabilities
- **Gemini Pro** - Comprehensive analysis and reasoning

#### **Multi-LLM Models** (via Intelligent Multi-LLM system)
- **Llama 3.2** - Natural conversation and creative tasks
- **DeepSeek** - Advanced reasoning and mathematical problem solving
- **Qwen** - Technical discussions and programming assistance

#### **User-Specific Models**
- **Personal Ollama Models** - User's own Ollama server models
- **Custom Configurations** - User-defined model preferences

## üöÄ Implementation

### **Frontend Components**

#### **ModelSwitcher.js**
```javascript
// Main component with intelligent model detection
<ModelSwitcher
    selectedModel={selectedModel}
    onModelChange={handleModelChange}
    isSidebarOpen={true}
    userId={userId}
/>
```

#### **Key Features:**
- **Dynamic model loading** from multiple sources
- **Real-time status checking** for model availability
- **Visual specialization indicators** (icons, colors, tags)
- **Responsive design** for different screen sizes

### **Backend Integration**

#### **API Endpoints:**
- `/api/multi-llm/status` - Get available Multi-LLM models
- `/api/user-ollama/models` - Get user's Ollama models
- `/api/user-ollama/status` - Check user's Ollama connection

#### **Chat Integration:**
```javascript
// Payload includes model selection
const payload = {
    query: userMessage,
    multiLLM: isMultiLLMEnabled,
    selectedModel: selectedModel
};
```

## üé® User Interface

### **Sidebar Integration**
- **Prominent placement** at the top of the sidebar
- **Collapsible details** for space efficiency
- **Visual status indicators** for model availability
- **Specialty tags** showing model capabilities

### **Model Information Display**
- **Model name and provider**
- **Description and capabilities**
- **Specialization tags** (Research, Technical, Creative, etc.)
- **Connection status** (Available, Limited, Unavailable)
- **Model size** (for Ollama models)

### **Visual Design**
- **Color-coded specializations:**
  - üîç Research (Purple)
  - üß† Reasoning (Blue)
  - üíª Technical (Orange)
  - üé® Creative (Pink)
  - üí¨ Chat (Green)

## üì± User Experience

### **Model Switching Flow**
1. **User selects model** from dropdown
2. **System validates** model availability
3. **Notification shows** successful switch
4. **Subsequent messages** use selected model
5. **Visual indicators** update throughout UI

### **Automatic Features**
- **Persistent selection** across sessions
- **Intelligent fallbacks** when models unavailable
- **Real-time status updates**
- **Performance optimization** with caching

## üîß Configuration

### **User Setup**
```javascript
// During signup - users can provide Ollama URL
{
    username: "user123",
    password: "password",
    ollamaUrl: "http://localhost:11434"  // Custom Ollama server
}
```

### **Model Preferences**
- **Default model selection**
- **Auto-enable Multi-LLM** for advanced models
- **Fallback preferences**
- **Performance settings**

## üéØ Benefits

### **For Users**
- **Optimal responses** for different query types
- **Personal model access** via Ollama integration
- **Seamless switching** without conversation interruption
- **Transparent routing** with clear model indicators

### **For Developers**
- **Modular architecture** for easy model addition
- **Intelligent routing** reduces manual model selection
- **Comprehensive error handling** and fallbacks
- **Performance monitoring** and analytics

## üìä Model Specializations

### **Gemini Flash**
- ‚úÖ **Research queries**
- ‚úÖ **Fact-finding**
- ‚úÖ **Web search integration**
- ‚úÖ **Current events**

### **Gemini Pro**
- ‚úÖ **Complex analysis**
- ‚úÖ **Comprehensive reasoning**
- ‚úÖ **Multi-step problems**
- ‚úÖ **Detailed explanations**

### **Llama 3.2** (via Ollama)
- ‚úÖ **Natural conversation**
- ‚úÖ **Creative writing**
- ‚úÖ **Storytelling**
- ‚úÖ **Casual chat**

### **DeepSeek**
- ‚úÖ **Mathematical reasoning**
- ‚úÖ **Logical analysis**
- ‚úÖ **Problem solving**
- ‚úÖ **Step-by-step solutions**

### **Qwen**
- ‚úÖ **Programming assistance**
- ‚úÖ **Code review**
- ‚úÖ **Technical discussions**
- ‚úÖ **System design**

## üîÑ Intelligent Routing

### **Automatic Detection**
When Multi-LLM is enabled, the system automatically:
1. **Analyzes conversation type**
2. **Selects optimal model**
3. **Routes query appropriately**
4. **Provides reasoning for selection**

### **Manual Override**
Users can manually select models for:
- **Specific preferences**
- **Testing different approaches**
- **Comparing model responses**
- **Personal workflow optimization**

## üõ†Ô∏è Technical Details

### **State Management**
```javascript
// Persistent model selection
const [selectedModel, setSelectedModel] = useState(() => {
    return localStorage.getItem('selectedModel') || 'gemini-flash';
});

// Multi-LLM enablement
const [isMultiLLMEnabled, setIsMultiLLMEnabled] = useState(() => {
    return JSON.parse(localStorage.getItem('multiLLMEnabled')) || false;
});
```

### **Model Change Handler**
```javascript
const handleModelChange = useCallback((model) => {
    setSelectedModel(model.id);
    
    // Enable Multi-LLM for advanced models
    if (model.isMultiLLM || model.isOllama) {
        setIsMultiLLMEnabled(true);
    }
    
    // Show success notification
    showNotification(`Switched to ${model.name}`);
}, []);
```

## üéâ Demo

Visit `/model-switcher-demo.html` for an interactive demonstration of the Model Switcher functionality.

## üöÄ Future Enhancements

### **Planned Features**
- **Model performance analytics**
- **Usage statistics and recommendations**
- **Custom model training integration**
- **Advanced routing algorithms**
- **Team/organization model sharing**

### **Integration Opportunities**
- **Voice model selection**
- **Context-aware auto-switching**
- **Model ensemble responses**
- **A/B testing capabilities**

## üìù Usage Examples

### **Research Query**
```
User selects: Gemini Flash
Query: "What are the latest developments in quantum computing?"
Result: Comprehensive research with web sources
```

### **Programming Help**
```
User selects: Qwen
Query: "Write a Python function to sort a dictionary by values"
Result: Technical code solution with explanations
```

### **Creative Writing**
```
User selects: Llama 3.2
Query: "Write a short story about a robot learning to paint"
Result: Creative, engaging narrative
```

### **Mathematical Problem**
```
User selects: DeepSeek
Query: "Solve this step by step: If 2x + 5 = 15, what is x?"
Result: Detailed step-by-step mathematical solution
```

---

The Model Switcher represents a significant advancement in AI interaction, providing users with unprecedented control over their AI experience while maintaining the simplicity and elegance of a single interface. üéØ
